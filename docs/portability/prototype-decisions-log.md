# Prototype Decisions Log

A structured record of every significant design, architecture, and technology decision made during SEND Data Browser prototype development. Each entry explains **what** was chosen, **why**, what alternatives existed, and the **impact on the Datagrok production port**.

This document is intended for the Datagrok developer(s) who will build the production plugin. It provides the rationale behind every prototype shortcut so they can distinguish deliberate design choices (keep) from prototype expedients (replace).

---

## 1. Architecture Decisions

| # | Decision | What We Chose | Why | Alternatives Considered | Impact on Datagrok Port |
|---|----------|---------------|-----|------------------------|------------------------|
| A1 | **Pre-computed vs live computation** | Pre-compute all statistics at build time via a Python generator script; serve results as static JSON files. | The prototype demonstrates one study. The frontend does not know or care whether JSON came from a live pipeline or a pre-computed file -- the UX is identical. This cut weeks of pipeline infrastructure work while keeping the statistical results real (scipy, pandas, scikit-posthocs running against actual .XPT data). | (a) Live computation on every API request. (b) Lazy computation with caching. (c) Database-materialized views. | **Must change.** Datagrok must compute on study import or on-demand with caching. The good news: the data schemas and frontend rendering do not change at all. The generator script serves as an executable specification of the computation pipeline -- port the math, discard the file I/O. |
| A2 | **File-based JSON storage vs database** | Annotations stored as JSON files on disk (`backend/annotations/{study_id}/{schema_type}.json`). No concurrency control, no transactions. | Simplest possible persistence for a single-user prototype. The annotation API contract (GET/PUT JSON payloads) was designed to be storage-agnostic -- swapping file I/O for database operations requires zero frontend changes. | (a) SQLite. (b) PostgreSQL. (c) Datagrok's built-in storage. | **Must change.** Replace `json.dump()`/`json.load()` in `annotations.py` with database operations. Add concurrency control, audit trail, and backup. The REST API contract and all React Query hooks survive unchanged. |
| A3 | **Single-study restriction** | `ALLOWED_STUDIES = {"PointCross"}` in backend config. Only one study is served. | The prototype exists to demonstrate the full user journey through one study, not to prove multi-study infrastructure. Restricting to one study eliminated study management, study switching state, and multi-tenant concerns. | (a) Serve all 16 studies in `send/`. (b) Dynamic study registration. | **Must change.** Remove the `ALLOWED_STUDIES` filter entirely. Remove the `PointCross` guard in `ContextPanel.tsx`. Studies should come from Datagrok's study management system. |
| A4 | **No authentication** | CORS `allow_origins=["*"]`, no auth middleware anywhere. Reviewer identity hardcoded as `"User"`. | Authentication adds zero value to a prototype that runs on localhost for design review. Every hour spent on auth is an hour not spent on the analysis UX that Datagrok developers need to evaluate. | (a) Basic auth. (b) JWT tokens. (c) Datagrok SSO integration. | **Must change.** Add Datagrok auth middleware. All API endpoints must validate tokens. Reviewer identity must come from auth context. This is Priority 1 in the migration guide. |
| A5 | **React + FastAPI standalone app vs Datagrok plugin** | Built as a standalone web app (React 19 + FastAPI) that replicates Datagrok's interaction patterns without depending on Datagrok. | The audience is Datagrok developers evaluating the design. A standalone app lets them see the full user journey, run it locally, and inspect the code without needing a Datagrok instance. It also allowed faster iteration -- React hot reload vs Datagrok plugin build cycle. | (a) Build directly as a Datagrok plugin from day one. (b) Hybrid: Datagrok shell with React panels. | **The entire frontend is replaced.** The standalone React app is a specification-by-example. Datagrok developers port the interaction patterns (three-panel layout, context panel behavior, cross-view links, color schemes) into Datagrok's native TypeScript SDK. The data schemas, API contracts, and statistical pipeline port directly. |
| A6 | **Static vs interactive charts** | Only 3 charts are truly interactive (signal heatmap, dose-response chart, severity heatmap). All other charts are pre-rendered as HTML/SVG by the generator script. | Interactive charting libraries are the riskiest time sink in frontend development. By limiting interactivity to the 3 charts that are core to user workflow (scanning, filtering, drilling), we saved ~4-6 hours and eliminated the riskiest implementation items. Static charts show real data -- they are just rendered at build time. | (a) All charts interactive. (b) All charts static. (c) Use a single charting library for everything. | **Recommend keeping this split in early Datagrok development.** Build the 3 interactive charts first using Datagrok's viewer framework. Defer custom visualizations (dose ladder, concordance panel, organ evidence matrix) to Phase 1C. The static HTML versions serve as visual specifications. |
| A7 | **Client-side filtering and sorting** | Analysis view grids use TanStack Table with client-side sorting and filtering. Domain views use server-side pagination. | Analysis view DataFrames are small enough (max ~1,400 rows for dose-response metrics) to fit entirely in the browser. Client-side filtering gives instant response. Domain views with raw .XPT data can be large, so those paginate server-side. | (a) Server-side filtering everywhere. (b) GraphQL for flexible queries. (c) Datagrok DataFrame filtering. | **Datagrok handles this natively.** Datagrok DataFrames support built-in filtering, sorting, and viewer linking. The prototype's client-side approach validated that the data volumes are small enough for in-memory operation -- Datagrok will be even more efficient since its DataFrames are already optimized for this. |
| A8 | **Pre-generated JSON views vs live API** | 8 pre-generated JSON files per study, served as static files by FastAPI. One HTML chart file. | The generator script is the single source of truth for all computed data. By writing to files, we can inspect, debug, and version the output without running the app. The backend becomes a trivial file server. | (a) Live API that computes on request. (b) Database-backed API with materialized views. (c) WebSocket streaming for large results. | **Architecture decision for Datagrok team.** Either (a) keep the generator pattern and run it on study import, or (b) compute views on-demand with caching. The frontend does not care -- it fetches from the same API endpoints either way. We recommend option (a) initially: compute on import, cache results, recompute on data change. |
| A9 | **Question-oriented views, not domain-oriented** | 5 analysis views organized by scientific questions ("What happened?", "Is it treatment-related?", "Which organs?", "What lesions?", "What is the NOAEL?"), not by SEND domains. | Scientists reason in layers: signal detection, causality, biology, evidence, decision. Organizing by domain (one tab per LB, MI, MA, etc.) forces the scientist to mentally integrate across tabs. Our views pre-integrate across domains and present synthesized answers. | (a) One view per SEND domain. (b) One view per endpoint. (c) Logbook-style event dashboards. (d) Individual-animal views as top-level. | **This is a core design decision -- keep it.** The 5-view model is the most important design contribution of the prototype. Each view answers a defensible scientific question. The view-specific DataFrames (study_signal_summary, dose_response_metrics, etc.) are the correct grain for each question. Do not regress to domain-oriented browsing as the primary surface. |
| A10 | **Vertical slices over horizontal layers** | Built one view completely end-to-end (View 1: Study Summary) before starting any other view. Each view includes grid + charts + context panel + insights. | This proves the entire architectural pattern works before replicating it. If the context panel or cross-view linking had a fundamental problem, we discovered it in View 1, not after building all 5 views. | (a) Build all grids first, then all charts, then all context panels. (b) Build the statistical pipeline for all views first. | **Strongly recommend for Datagrok.** The spec's phasing (Phase 1B: one view end-to-end, Phase 1C: remaining views) follows the same principle. Validate view switching, Sticky Meta binding, and data sync with one view before scaling to five. |

---

## 2. Technology Decisions

| # | Decision | What We Chose | Why | Alternatives | Port Impact |
|---|----------|---------------|-----|-------------|-------------|
| T1 | **Frontend framework** | React 19 + TypeScript (strict mode) + Vite | Industry-standard, fast hot reload, excellent TypeScript support, largest ecosystem of UI component libraries. The prototype needed to be built fast and be readable by any web developer. | (a) Vue 3. (b) Svelte. (c) Datagrok's native panels. (d) Vanilla TypeScript. | **Entirely replaced.** Datagrok plugins are TypeScript, but use Datagrok's own UI framework (grok.shell, TableView, etc.), not React. The React code is reference implementation only. Component logic and state patterns are the portable parts. |
| T2 | **CSS framework** | TailwindCSS v4 with custom Datagrok color theme in `index.css` | Utility-first CSS enabled rapid prototyping without writing custom stylesheets. The Datagrok color theme in `index.css` keeps the look consistent with Datagrok's native appearance. | (a) CSS Modules. (b) Styled Components. (c) Plain CSS. | **Not ported.** Datagrok has its own styling system. The color values (p-value reds/greens, signal score gradients, severity yellows/reds, dose group blues) are portable -- they are defined in the spec at `severity-colors.ts` and must be replicated exactly in Datagrok. |
| T3 | **UI component library** | shadcn/ui (Radix UI primitives + CVA) | High-quality accessible components (accordion, dialog, dropdown, tooltip) with full style control. No vendor lock-in -- components are copied into the project, not imported from a package. | (a) Material UI. (b) Ant Design. (c) Custom components. | **Not ported directly.** The component patterns (accordion panes in context panel, dropdown filters, dialog for NOAEL edit) are the portable concepts. Datagrok has native equivalents for most of these (accordions, dialogs, dropdowns). |
| T4 | **Data table library** | TanStack React Table v8 | Headless table library with full control over rendering, client-side sorting, column definitions, and selection state. No opinionated UI -- we style it ourselves. | (a) AG Grid. (b) React Data Grid. (c) Custom table. | **Replaced by Datagrok's Grid viewer.** Datagrok's built-in grid is far more capable (millions of rows, column types, formula columns, semantic type rendering). Column definitions and color-coding rules transfer as configuration. |
| T5 | **Server state management** | TanStack React Query (5-minute stale time) | Handles caching, refetching, loading states, and error states for all API calls. Decouples data fetching from component rendering. | (a) SWR. (b) Redux Toolkit Query. (c) Manual fetch + useState. | **Not applicable.** Datagrok handles data loading through its own DataFrame infrastructure. The stale-time and caching patterns are useful reference for any client-side caching Datagrok implements. |
| T6 | **Charting library** | Recharts (for the 3 interactive charts) | React-native, declarative, sufficient for line charts, bar charts, and scatter plots. Not the most powerful, but fast to implement and easy to customize. | (a) ECharts. (b) Plotly. (c) D3 direct. (d) Nivo. | **Replaced by Datagrok viewers.** Datagrok has built-in scatter plot, line chart, bar chart, and heatmap viewers. The chart configurations (axes, colors, series grouping) transfer as specifications. For the signal heatmap and dose ladder, custom JsViewers may be needed. |
| T7 | **Backend framework** | FastAPI + uvicorn | Async Python web framework with automatic OpenAPI docs, Pydantic validation, and fast development cycle. Python was required for the statistical pipeline (scipy, pandas). | (a) Flask. (b) Django REST. (c) Express.js (would need Python subprocess for stats). | **Backend is replaced entirely.** Datagrok's server-side scripting runs Python directly. The statistical computation code (scipy, pandas, pyreadstat) ports as-is. The REST API layer is not needed -- Datagrok has its own data serving infrastructure. |
| T8 | **Statistical libraries** | pandas, scipy.stats, scikit-posthocs, pyreadstat | Standard scientific Python stack. scipy for ANOVA, Fisher's exact, Kruskal-Wallis. scikit-posthocs for Dunnett's test, Jonckheere-Terpstra trend test. pyreadstat for .XPT file reading. | (a) R via rpy2 (more tests available, but adds dependency). (b) statsmodels (less complete for posthoc tests). (c) JavaScript statistics libraries (jStat -- too limited). | **Ports directly, but verify R availability.** If Datagrok can call R scripts, use R's `multcomp` and `DescTools` packages for more robust statistical tests (especially Dunnett's). If R is not available, the Python stack works. This is open question #6 in the spec. |
| T9 | **Routing** | React Router v7 | Standard React routing. Routes map to views. URL reflects current view and study. | (a) TanStack Router. (b) Next.js routing. (c) No routing (single page with state). | **Not applicable.** Datagrok has its own navigation model (grok.shell, view switching). The URL structure is not portable, but the route-to-view mapping documents which views exist. |
| T10 | **Validation engine** | Custom Python engine (YAML rules + pandas checks + pydantic models) | Lightweight, no external dependencies (no CDISC CORE engine, no CDISC Library API key). Operates on in-memory DataFrames. Output format matches the frontend's 6 evidence renderers exactly. | (a) CDISC CORE engine (MIT license, Python, but requires API key and Dask). (b) Pinnacle 21 (commercial). (c) SAS-based validation. | **Ports directly to Datagrok.** The engine's structure (YAML config + Python checks + pandas) maps directly to Datagrok's scripting model. Before production: have a SEND domain expert review every rule, compare against Pinnacle 21 output with 5-10 real studies, and obtain official SENDIG 3.1 metadata from CDISC Library. |

---

## 3. Data & Computation Decisions

| # | Decision | What We Chose | Why | Alternatives Considered | Port Impact |
|---|----------|---------------|-----|------------------------|------------------------|
| D1 | **Signal score formula and weights** | Composite score: `0.30 * w_stat + 0.30 * w_trend + 0.25 * w_effect + 0.15 * w_bio`. Hard-coded weights. | These weights reflect the relative importance of statistical significance, trend evidence, effect size magnitude, and biological plausibility in toxicology signal detection. The formula is defined in spec section 10.7.1. | (a) Equal weights (0.25 each). (b) User-configurable weights per study type. (c) ML-derived weights. | **Keep defaults, but make configurable.** The weights are reasonable starting points but need tuning after testing with diverse study types (carcinogenicity studies may weight trend higher; acute studies may weight effect size higher). The spec flags this as a Phase 2+ candidate (item #159 in TBD registry). |
| D2 | **Rule priority bands** | 16+ canonical rules organized into 4 priority bands: 900+ (NOAEL/decision), 800-899 (target organ identification), 600-799 (study-scope signals), 400-599 (modifiers/caveats), 200-399 (review flags). | Priority bands map directly to UI zones in the Findings mode. Higher-priority rules appear in more prominent positions. Conflict resolution uses priority ordering -- when rules contradict, the higher-priority rule wins. | (a) Flat priority (all rules equal). (b) Two tiers only (critical vs informational). (c) User-defined priority. | **Keep the band system.** The priority-to-UI-zone mapping is a core UX design principle. The specific priority values may need tuning, but the 5-band structure should be preserved in Datagrok. |
| D3 | **Adversity determination thresholds** | Hard-coded thresholds: MI severity cutoff, LB delta threshold, OM percentage threshold, BW percentage threshold. Defined in spec section 10.9. | These thresholds determine when a finding crosses from adaptive/incidental to adverse -- the single most impactful scientific judgment in the app. Hard-coding was acceptable for the prototype because thresholds should be reviewed by a toxicologist before production anyway. | (a) User-configurable thresholds via settings dialog. (b) Study-type-specific preset profiles. (c) No automated adversity -- manual annotation only. | **Make configurable.** This is the highest-priority configurability item. Different study types and sponsors have different adversity criteria. Implement a settings dialog where the study director can adjust thresholds. Keep the current values as defaults. Spec flags this as TBD item #29. |
| D4 | **NOAEL computation method** | Automated: find the highest dose with no adverse effects (based on adversity flags). When annotation overrides exist (ToxFinding.adversity), use the override. Confidence score penalized by -0.2 per factor (borderline findings, peer review disagreements, insufficient evidence). | NOAEL must be traceable to evidence in one click. The automated method ensures every NOAEL has an audit trail. The override mechanism respects that NOAEL is ultimately a scientific judgment, not a statistical output. | (a) Fully manual NOAEL entry (no automation). (b) Statistical-only NOAEL (no override). (c) Multiple NOAEL candidates with probability scores. | **Keep the auto-compute + override pattern.** The Datagrok implementation should compute NOAEL automatically on data load and allow override via the NOAEL configuration dialog (spec section 14). Confidence score penalties may need calibration -- the -0.2 per factor is a starting point. |
| D5 | **Organ system mapping approach** | Static mapping table: tissue name to organ system, lab test code to organ system. Unmapped terms fall to "Other". | The mapping is essential for cross-domain integration (View 3: Target Organs). A lab ALT result and a histopath liver finding must resolve to the same organ system ("Hepatobiliary") for convergent evidence scoring to work. | (a) Dynamic mapping via ontology lookup. (b) NLP-based tissue-to-organ mapping. (c) No mapping -- keep domains separate. | **Extend the mapping table.** The prototype mapping covers common terms. Production needs a more comprehensive mapping, ideally derived from SEND controlled terminology and INHAND vocabulary. Check how many terms fall through to "Other" with real data (TBD item #20). |
| D6 | **Controlled terminology approach** | Embedded YAML files with common CDISC CT values. Unknown values generate Warning, not Error. | A self-contained engine with no external dependencies. The CT lists are approximate but cover the most common values for key codelists (SEX, SPECIES, STRAIN, ROUTE, SPECIMEN, RESULT_CATEGORY, BASELINE_FLAG). | (a) CDISC Library API for live CT lookup. (b) Full CDISC CT package embedded. (c) No CT validation. | **Upgrade to official CDISC CT.** Obtain the official CDISC Controlled Terminology package for SEND and regenerate the metadata YAML files. This is a data task, not an engineering task. The engine structure handles it without code changes. |
| D7 | **Dose group mapping** | Hard-coded: `ARMCD_TO_DOSE_LEVEL = {"1": 0, "2": 1, "3": 2, "4": 3}`. Recovery arms: `{"1R", "2R", "3R", "4R"}`. | The PointCross study uses simple sequential ARMCD values. This mapping was sufficient for the prototype's single study. | (a) Derive dynamically from TX/DM domains. (b) User-configured per study. (c) Infer from dose values in EX domain. | **Must change.** The hard-coded mapping breaks for any study with non-sequential ARMCD values, satellite groups, TK groups, or recovery groups. Derive dose level mapping dynamically from the study's TX and DM domains. This is TBD item #18. |
| D8 | **Statistical test selection** | Per spec section 9.4: parametric (ANOVA + Dunnett's) for continuous data with normal distribution; non-parametric (Kruskal-Wallis + JT trend) for non-normal or ordinal; Fisher's exact for incidence; Cochran-Armitage for incidence trend. | Standard toxicology statistical methods. The selection follows regulatory guidance (ICH S3A) and common practice in preclinical study reports. | (a) Always non-parametric (simpler, fewer assumptions). (b) Bayesian methods. (c) No formal statistics -- descriptive only. | **Keep the test selection logic.** The statistical methods are correct for preclinical toxicology. If R is available in Datagrok, use R's `multcomp::glht()` for Dunnett's (more robust for unbalanced designs) and `DescTools::JonckheereTerpstraTest()` for JT trend. Python implementations are adequate but R is the gold standard for these tests. |

---

## 4. UX Decisions

| # | Decision | What We Chose | Why | Alternatives Considered | Port Impact |
|---|----------|---------------|-----|------------------------|------------------------|
| U1 | **Three-panel layout** | Left toolbox (~200px, tree nav) + center content (grid + charts + filters) + right context panel (~300px, accordion panes that update on row selection). | Replicates Datagrok's native layout model. The three-panel pattern is familiar to Datagrok users and matches how scientists work: navigate (left), examine (center), inspect detail (right). | (a) Two-panel (no toolbox tree). (b) Tab-based navigation. (c) Dashboard layout with cards. | **This IS the Datagrok layout.** The prototype was deliberately built to mirror Datagrok's grok.shell layout. The toolbox tree maps to Datagrok's sidebar. The context panel maps to Datagrok's property panel. The center area maps to the active TableView. Direct translation. |
| U2 | **Context panel as primary detail surface** | Right sidebar with accordion panes. Updates on grid row click (~200ms debounce). First pane expanded by default, others collapsed. Rule-based insights always in first pane. | "The context panel IS the product." If the grid works but the context panel does not update on row click with insights, stats, and cross-view links, the prototype fails. This is where the scientific synthesis happens -- not in the grid itself. | (a) Modal dialogs for detail. (b) Inline row expansion. (c) Bottom panel (master-detail split). | **Critical to preserve.** The context panel behavior is the most important UX pattern to replicate. Accordion panes, lazy loading, and semantic pane content per view are the differentiating features. Map each pane set (spec section 11.4-11.8) to Datagrok's info panel system. |
| U3 | **Tree navigation in toolbox** | Hierarchical tree: Analysis Views (5 items) + Domains (8 items). Click item to switch the center content. | Matches Datagrok's browsing tree pattern. Scientists scan the tree to understand what is available, then click to navigate. The tree shows the complete view inventory at a glance. | (a) Tab bar across the top. (b) Dropdown selector. (c) Breadcrumb-only navigation. | **Maps to Datagrok's sidebar tree.** Datagrok natively supports tree navigation in the sidebar. The prototype's `BrowsingTree` component documents the exact tree structure and item labels to replicate. |
| U4 | **Cross-view linking** | Clickable text in context panel panes (e.g., "View in dose-response") switches to the target view AND applies a filter on the target endpoint/organ. | This is the UX pattern Datagrok developers most need to see. Cross-view linking demonstrates that the 5 views are not isolated tabs but an interconnected analytical surface. A finding in View 1 leads to causal investigation in View 2 leads to organ assessment in View 3. | (a) No linking -- user manually navigates. (b) Linked views side-by-side. (c) Breadcrumb trail only. | **Must work in Datagrok.** This is spec open question #2: "Correct pattern for info pane link to switch view + apply filter?" The prototype proves the interaction works. The Datagrok developer must find the correct implementation pattern for cross-view navigation (Phase 0 validation). |
| U5 | **Dual-mode Signals tab** | The Study Summary view's Signals tab has two modes: Findings mode (structured text synthesis) and Heatmap mode (organ-grouped signal matrix). Persistent Decision Bar across both modes. Shared selection state and context panel. | Scientists need both: a readable summary (Findings) for orientation and triage, and a data-dense matrix (Heatmap) for detailed signal scanning. The dual-mode pattern avoids information overload while keeping all data accessible. | (a) Single mode with everything visible. (b) Two separate views. (c) Findings only (no heatmap). | **Recommend preserving this pattern.** The Findings mode is the most opinionated and valuable UX innovation. It synthesizes 975 rule results into a readable narrative. The Heatmap mode provides the traditional analytical surface. Datagrok should implement both, sharing selection state. |
| U6 | **Fix tier system for validation** | Three tiers: Tier 1 (Accept as-is -- value is intentional, user provides justification), Tier 2 (Simple correction -- known fix, apply suggestion), Tier 3 (Script fix -- batch logic, opens script dialog). Two independent status tracks: fix status and review status. | Validation findings range from trivial (wrong case) to complex (derived calculation errors). A single "fix" action does not fit all cases. The tier system tells the user what kind of attention each finding needs. Independent fix/review tracks separate "what happened to the data" from "human sign-off". | (a) Single fix/status track. (b) Binary accept/reject. (c) No fix guidance -- just report issues. | **Keep the tier system.** The 3-tier model maps well to real SEND validation workflows. Tier 1 maps to "acceptable deviation" in regulatory submissions. Tier 2 maps to automated data cleaning. Tier 3 maps to programmatic fixes that a data manager writes. The independent status tracks are essential for audit trails. |
| U7 | **Annotation-only fix scripts** | Fix scripts compute before/after previews but do NOT modify source .XPT data. "Applying" a fix only updates the annotation status. | Modifying source .XPT files is dangerous and irreversible in a prototype. The preview-only approach demonstrates the UX (user sees what would change) without risking data corruption. In production, the decision of whether to modify source data is a policy decision, not a technical one. | (a) Actually modify .XPT files. (b) Write corrected copies. (c) Maintain a correction layer on top of source data. | **Production policy decision.** Datagrok team must decide: (a) modify source files (risky, needs backup), (b) maintain a correction overlay (complex but safe), or (c) export corrected copies (simple but creates file management burden). The annotation-only pattern works for audit purposes even if data is not modified. |
| U8 | **Insights as stated facts, not suggestions** | Rule outputs use declarative language: "This finding IS treatment-related." Not "may be" or "suggests." | Per spec section 10.10, the system makes interpretive claims based on evidence, presented as stated facts. The scientist's job is to review and override if they disagree, not to interpret hedged language. This mirrors how rule-based expert systems work in regulatory contexts. | (a) Hedged language ("may be treatment-related"). (b) Probability scores without interpretation. (c) No interpretive claims -- just statistics. | **Core design principle -- keep it.** Stated facts with evidence references and override capability is the correct pattern for regulatory toxicology software. The scientist trusts or overrides; the system does not equivocate. |
| U9 | **No breadcrumb navigation in context panel** | Back/forward icon buttons (`< >`) at the top of the context panel. No breadcrumb trail. | Mirrors Datagrok's native context panel behavior. Breadcrumbs consume vertical space in a panel that is already narrow. Back/forward is sufficient for the two-level depth (mode 1: rule summary, mode 2: issue detail). | (a) Breadcrumb trail. (b) Nested panels. (c) Modal for detail level. | **Matches Datagrok.** Datagrok's property panel uses a similar back/forward pattern. If Datagrok adds breadcrumbs to its panel system later, adapt accordingly. |
| U10 | **Sentence case for all UI text** | Sentence case everywhere except: L1 page/view headers (Title Case), dialog headers (Title Case), and context action labels (Title Case). Buttons are sentence case except OK, SAVE, RUN. | Consistent casing convention reduces design decisions and creates a professional, readable interface. Sentence case is easier to read for the dense text found in scientific applications. | (a) Title Case everywhere. (b) UPPERCASE for all labels. (c) No convention (inconsistent). | **Adopt or adapt.** The casing convention is documented in CLAUDE.md. Datagrok may have its own convention -- if so, use theirs. If not, adopt this one. Consistency matters more than the specific choice. |

---

## 5. What Was Deliberately Stubbed or Skipped

Reference: `CLAUDE.md` section "Demo/Stub/Prototype Code -- Production Migration Guide"

### Priority 1: Infrastructure Dependencies (Must Replace)

| ID | What Was Stubbed | How It Works in Prototype | Why It Was Stubbed | Datagrok Recommendation |
|----|-----------------|--------------------------|-------------------|------------------------|
| P1.1 | **Authentication & authorization** | CORS `allow_origins=["*"]`, no auth middleware, reviewer identity hardcoded as `"User"`. | Auth adds zero value for a localhost design review. Every auth hour is an analysis UX hour lost. | Implement Datagrok SSO. All endpoints validate tokens. Reviewer identity from auth context. |
| P1.2 | **Database for annotations** | JSON files on disk: `backend/annotations/{study_id}/{schema_type}.json`. No concurrency, no transactions. 4 schema types: tox-findings, pathology-reviews, validation-issues, validation-records. | Simplest persistence for single-user prototype. API contract designed to be storage-agnostic. | Replace file I/O with database tables. The GET/PUT REST API contract and all React Query hooks survive unchanged -- zero frontend changes needed. |
| P1.3 | **Multi-study support** | `ALLOWED_STUDIES = {"PointCross"}`. Frontend shows "demo entry" message for non-PointCross studies. | Prototype demonstrates one study end-to-end. Multi-study adds study management complexity without UX insight. | Remove filter. Remove PointCross guard. Studies from Datagrok's study management. |

### Priority 2: Hardcoded Demo Data (Must Remove)

| ID | What Was Stubbed | How It Works in Prototype | Why It Was Stubbed | Datagrok Recommendation |
|----|-----------------|--------------------------|-------------------|------------------------|
| P2.1 | **Demo studies on landing page** | 4 fake studies (DART-2024-0091, CARDIO-TX-1147, ONCO-MTD-3382, NEURO-PK-0256) with fabricated metadata. Real studies get hardcoded `validation: "Pass"`. | The landing page needs to look populated for design review. One real study alone looks empty. | Delete `DEMO_STUDIES` array and all `isDemo` logic. Validation status from actual validation results API. |
| P2.2 | **Hardcoded validation rules & records** | 8 fake rules, 40+ fake affected records, 3 mock fix scripts as TypeScript constants in `ValidationView.tsx`. | The validation UI was built before the validation engine existed. Fake data proved the UI design works. A real engine was later specified in `validation-engine-build-prompt.md`. | Replace with API calls to the real validation engine. The validation engine build prompt has full instructions. The UI components (rules table, affected records, fix tiers, script dialog) are reusable -- only the data source changes. |

### Priority 3: Stub Features (Implement or Remove)

| ID | What Was Stubbed | How It Works in Prototype | Why It Was Stubbed | Datagrok Recommendation |
|----|-----------------|--------------------------|-------------------|------------------------|
| P3.1 | **Import section** | Non-functional UI on landing page: drop zone does not accept drops, browse button shows `alert()`, all inputs disabled, import button disabled with tooltip "not available in prototype". | Import workflow requires file upload infrastructure, XPT parsing pipeline, and study registration -- none of which exist in the prototype. The stub shows what the UX should look like. | Replace with Datagrok's study import workflow, or implement real file upload + XPT parsing + study registration pipeline. |
| P3.2 | **Export (CSV/Excel)** | `alert("CSV/Excel export coming soon.")` in context menu and StudyInspector. | Export requires backend serialization logic. Low UX insight value for prototype. | Implement actual CSV/Excel export for study data, analysis results, and reports. |
| P3.3 | **Share / Re-validate / Delete** | Always-disabled context menu items. No implementation. | These require multi-user infrastructure (sharing), validation trigger (re-validate), and confirmation UX + backend delete logic. | Implement with proper confirmation dialogs and backend support. |
| P3.4 | **Documentation link** | `alert("Documentation is not available in this prototype.")` | No documentation exists yet. | Link to actual product documentation. |
| P3.5 | **Feature flags** | `ANALYSIS_TYPES` and `ANALYSIS_VIEWS` arrays with `implemented` boolean flags. `PlaceholderAnalysisView` catch-all for unimplemented types. | Allowed incremental development -- views could be added one at a time while unfinished ones showed a placeholder. | Remove flags if all views are implemented. Or keep as a gating mechanism for staged rollout. |

### Priority 4: Architecture Decisions (Evaluate)

| ID | What Was Stubbed | How It Works in Prototype | Why It Was Stubbed | Datagrok Recommendation |
|----|-----------------|--------------------------|-------------------|------------------------|
| P4.1 | **Generator pipeline** | CLI tool reads .XPT, computes statistics, writes 8 JSON files + 1 HTML chart. Backend serves files directly via `json.load()`. | Compute-once-serve-static is the simplest architecture. One study, one developer, no concurrency. | Decide: (a) keep generator pattern, run on import; or (b) compute on-demand with caching. Frontend does not care either way. |
| P4.2 | **File-based caching** | XPT domains cached as CSV. Adverse effects cached as JSON. Freshness checked against .XPT file mtime. | Simple, effective for single-study prototype. No external dependencies. | Replace with Datagrok's data infrastructure or a proper cache layer (Redis, database materialized views). |

### Priority 5: Hardcoded Configuration (Parameterize)

| ID | What Was Stubbed | How It Works in Prototype | Why It Was Stubbed | Datagrok Recommendation |
|----|-----------------|--------------------------|-------------------|------------------------|
| P5.1 | **Dose group mapping** | `ARMCD_TO_DOSE_LEVEL = {"1": 0, "2": 1, "3": 2, "4": 3}`. Only works for PointCross. | The prototype only serves one study with simple sequential ARMCDs. | Derive dynamically from TX/DM domains. Make configurable per study. |
| P5.2 | **Skip folders** | `SKIP_FOLDERS = {"JSON-CBER-POC-Pilot-Study3-Gene-Therapy", "SENDIG3.1.1excel"}`. | Known bad folders in the `send/` directory that crash the XPT reader. | Not needed if study discovery is replaced by Datagrok's study management. |
| P5.3 | **Data directory** | `SEND_DATA_DIR` defaults to `C:\pg\pcc\send`. | Hardcoded to the developer's machine. Env-overridable. | Use Datagrok's file storage system. |
| P5.4 | **Domain defaults in generator** | Hardcoded `domain_defaults` mapping domains to default organ systems. Hardcoded list of relevant XPT domains. | Sufficient for PointCross. The mappings are reasonable defaults for most studies. | Make domain-organ mappings configurable. Consider SEND controlled terminology for domain discovery. |

---

## 6. Open Questions for Datagrok Developer

These are the 13 must-resolve questions from spec section 18.2. Each must be answered before the relevant implementation phase begins. The prototype does not answer these -- they are Datagrok-platform-specific.

| # | Question | Context | Why It Matters | Affects Phase |
|---|----------|---------|---------------|--------------|
| 1 | **Does swapping TableViews in grok.shell preserve state (filters, scroll, selection)?** | The prototype uses React Router for view switching with state preserved in React contexts. Datagrok must swap entire TableViews when the user clicks between the 5 analysis views. | If state is lost on view switch, the navigation model needs redesign. Users expect to switch between views and return to their previous position. If Datagrok cannot preserve per-view state, consider tabs instead of single-view switching. | Phase 0 |
| 2 | **What is the correct pattern for cross-view navigation (info pane link -> switch view + apply filter)?** | The prototype implements this as: navigate to new route + set filter state in React context + target view reads filter on mount. Datagrok needs: programmatic view switch + programmatic filter application on the target DataFrame. | Cross-view links are the most important interaction pattern in the prototype. Every context panel has "View in dose-response" or "View target organ" links. If this pattern cannot work in Datagrok, the entire analytical flow degrades. | Phase 0 |
| 3 | **Can Sticky Meta values be read into DataFrame columns reactively (event-driven)?** | When a user annotates a finding as "treatment-related" via ToxFinding Sticky Meta, that annotation must immediately propagate to the analysis DataFrames and rule engine outputs. The prototype uses React state + React Query invalidation. | If annotations do not propagate reactively, the user must manually refresh views after annotating. This breaks the flow where a scientist annotates in View 2 and immediately sees the impact on NOAEL in View 5. | Phase 0 |
| 4 | **Does project save/restore preserve all DataFrames, viewer layouts, grid configurations, and project metadata?** | The prototype uses localStorage for annotation persistence. Datagrok projects must save/restore the complete study state including computed DataFrames, viewer layouts, annotation state, and NOAEL overrides. | If save/restore loses state, users will not trust the app for multi-session work. Determine exactly what persists and what needs custom persistence code. | Phase 0 |
| 5 | **Can Datagrok detect source file changes and trigger re-import?** | The prototype has no data sync. Once JSON is generated, it is static. In production, source .XPT files might be updated (data fixes, new domain additions). | If auto-detection is not available, implement manual re-import only. The prototype's file-mtime caching in `xpt_processor.py` shows one approach. | Phase 0 |
| 6 | **Is R available from the TypeScript layer? What is the latency?** | The prototype uses Python (scipy, scikit-posthocs) for all statistics. R has more mature implementations of Dunnett's test (multcomp), JT trend test (DescTools), and Williams' test. | Determines whether statistics run server-side (R) or client-side (JS library, limited). If R is available with acceptable latency (<500ms per call), use R for Dunnett's and JT. If not, the Python implementations work. | Phase 1B |
| 7 | **Which statistical libraries (R packages or JS equivalents) are available in Datagrok?** | The prototype needs: Dunnett's post-hoc test, Jonckheere-Terpstra trend test, Cochran-Armitage trend test, Fisher's exact test, Kruskal-Wallis, ANOVA, Cohen's d. | The specific library availability determines whether the Python code can be used as-is, needs porting to R, or needs porting to JavaScript. | Phase 1B |
| 8 | **How should the signal heatmap be implemented: pre-pivot DataFrame, custom JsViewer, or built-in column mapping?** | The prototype uses a custom React component (`OrganGroupedHeatmap`) that renders a pivoted endpoint-by-dose matrix with color-coded cells, organ grouping, and collapsible sections. | The signal heatmap is the single most important visualization. If Datagrok's built-in heatmap viewer can handle pivot, custom cell text, row grouping, and organ-based collapsing, use it. If not, build a custom JsViewer. | Phase 1B |
| 9 | **Can Datagrok conditionally switch viewer type based on data type (continuous vs incidence)?** | The dose-response chart (View 2) must show line+scatter for continuous endpoints (BW, LB) but stacked bar for incidence endpoints (MI, MA). The prototype uses conditional rendering in React. | This is a UX requirement: the same chart position shows different chart types depending on the selected endpoint's data type. | Phase 1C |
| 10 | **How should the grade distribution JSON be unpacked: at DataFrame creation or in a custom viewer?** | Histopathology grade distributions are stored as JSON objects within a DataFrame column (`GRADE_DISTRIBUTION`). The prototype unpacks them in the React component at render time. | Nested JSON in DataFrame columns is unusual. Determine whether Datagrok handles this natively, needs a custom viewer, or requires unpacking during DataFrame creation. | Phase 1C |
| 11 | **How should the subject view be implemented: expanded context panel, docked panel, or modal?** | The prototype does not fully implement the subject view. The spec describes an individual animal profile with timeline, BW chart, findings summary, and contribution to study-level signals. | This is the "confidence check" view -- scientists click an animal ID to verify that an individual animal's story makes biological sense. The implementation approach affects how it integrates with the existing layout. | Phase 1C |
| 12 | **How should reports be rendered: HTML template to PDF (headless browser), R Markdown, or other?** | The prototype generates a standalone HTML page with inline CSS and opens it in a new tab. Production needs PDF export with proper formatting. | Report generation is Phase 1D. The approach depends on what rendering infrastructure Datagrok provides (headless browser, PDF library, R Markdown). | Phase 1D |
| 13 | **Does project sharing preserve full data + layouts for recipients without source file access?** | The prototype has no sharing. In production, a study director needs to share their analysis (with annotations, NOAEL override, viewer layouts) with a pathologist who may not have access to the original .XPT files. | If sharing requires source file access, the sharing model needs simplification. Determine whether Datagrok projects are self-contained. | Phase 1D |

---

## Appendix: Summary of What Is Real vs What Must Change

This table provides a quick reference for the Datagrok developer to understand which prototype components carry forward versus which are throwaway.

| Component | Prototype Status | Carries Forward To Datagrok? |
|-----------|-----------------|------------------------------|
| Statistical analysis pipeline (generator/) | Real computation on real data | **YES** -- port the math (pandas, scipy, pyreadstat). Discard the file I/O wrapper. |
| 7 view-specific DataFrame schemas | Real schemas, documented in spec section 9.6 | **YES** -- these are the data contracts. Column names, data types, grain all carry over. |
| Signal scoring formula (R01-R16 rules) | Real rules derived from data patterns | **YES** -- the rule definitions, priority bands, and conflict resolution logic all carry over. |
| Organ system mapping table | Real mapping, covers common terms | **YES** -- extend with more terms, but keep the structure. |
| Color scheme definitions | Exact hex values from spec section 12.3 | **YES** -- these must be replicated exactly in Datagrok viewers. |
| Context panel pane specifications | Real pane content per spec section 11 | **YES** -- pane structure, content, and update-on-selection behavior carry over. |
| Cross-view link targets | Real navigation targets | **YES** -- the link targets (which view, which filter) carry over. |
| Annotation API contract | Real REST API (GET/PUT JSON) | **PARTIAL** -- the contract shape carries over; the REST transport is replaced by Datagrok's data layer. |
| Validation engine (YAML rules + Python checks) | Real engine, real rules, real affected records | **YES** -- ports directly to Datagrok's Python scripting. Extend rules and metadata. |
| HTML report generator | Real report, fetches live data | **PARTIAL** -- report structure and content carry over; rendering mechanism changes (PDF instead of HTML). |
| React components | Real UI, fully interactive | **NO** -- reference implementation only. Port the interaction patterns, not the React code. |
| TailwindCSS / shadcn/ui | Prototype styling | **NO** -- Datagrok has its own styling system. Only color values carry over. |
| FastAPI backend | Prototype infrastructure | **NO** -- replaced by Datagrok's server-side infrastructure. |
| File-based storage | Prototype expedient | **NO** -- replace with database. |
| Authentication system | Does not exist | **BUILD NEW** -- Datagrok SSO integration needed. |
| Multi-study management | Restricted to one study | **BUILD NEW** -- Datagrok's study management system. |
